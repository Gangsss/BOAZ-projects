{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_seoul = glob.glob('review_*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review_busan_70028.json', 'review_daegu_33689.json', 'review_jeju_143957.json', 'review_jeonju_13647.json', 'review_seoul_326990.json']\n"
     ]
    }
   ],
   "source": [
    "print(lst_seoul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "with open('review_seoul_326990.json','r',encoding='utf-8') as fp:\n",
    "    tmp_dic = json.load(fp)\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = {} # 데이터 모두 합치기\n",
    "for el_seoul in lst_seoul:\n",
    "    with open(el_seoul,'r',encoding='utf-8') as fp:\n",
    "        tmp_dic = json.load(fp)\n",
    "    reviews.update(tmp_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28760"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'collection_tag': None,\n",
       " 'comments': '친구들이랑 6명이 갔는데\\n숙소 위치도 너무 좋고 방도 깨끗해서\\n다들 맘에 들어했어요!!\\n에어비엔비를  처음 이용해봤는데\\n너무 좋아서 다음번에도 이용할것같아요\\n해운대해변이랑도 걸어갈수 있고\\n연락도 너무 잘됐어요',\n",
       " 'created_at': '2016-10-08T23:57:42Z',\n",
       " 'id': 106917581,\n",
       " 'language': 'ko',\n",
       " 'localized_date': '2016년 10월',\n",
       " 'rating': 5,\n",
       " 'response': '',\n",
       " 'reviewee': {'deleted': False,\n",
       "  'first_name': 'Jaewook',\n",
       "  'host_name': 'Jaewook',\n",
       "  'id': 38530724,\n",
       "  'is_superhost': False,\n",
       "  'picture_url': 'https://a0.muscache.com/im/users/38530724/profile_pic/1436955527/original.jpg?aki_policy=profile_x_medium',\n",
       "  'profile_path': '/users/show/38530724'},\n",
       " 'reviewer': {'deleted': False,\n",
       "  'first_name': 'Joohee',\n",
       "  'host_name': 'Joohee',\n",
       "  'id': 96535213,\n",
       "  'is_superhost': False,\n",
       "  'picture_url': 'https://a0.muscache.com/im/pictures/32cbca58-85f2-47e5-b09c-a5ead7662304.jpg?aki_policy=profile_x_medium',\n",
       "  'profile_path': '/users/show/96535213'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['10030136'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = []\n",
    "for id,list in reviews.items() :\n",
    "    for lis in list :\n",
    "        lang.append(lis['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581511"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ko', 276891),\n",
       " ('en', 238308),\n",
       " ('zh', 18580),\n",
       " ('zh-TW', 17944),\n",
       " ('zh-CN', 16200),\n",
       " ('ja', 6027),\n",
       " ('fr', 2362),\n",
       " ('ru', 1177),\n",
       " ('und', 917),\n",
       " ('es', 879),\n",
       " ('de', 789),\n",
       " ('th', 631),\n",
       " ('nl', 140),\n",
       " ('it', 108),\n",
       " ('id', 90),\n",
       " ('pt', 83),\n",
       " ('sv', 75),\n",
       " ('no', 48),\n",
       " ('da', 45),\n",
       " ('pl', 37),\n",
       " ('fi', 37),\n",
       " ('vi', 25),\n",
       " ('ms', 22),\n",
       " ('tr', 17),\n",
       " ('cs', 14),\n",
       " ('ca', 13),\n",
       " (None, 8),\n",
       " ('ar', 6),\n",
       " ('bg', 6),\n",
       " ('hu', 6),\n",
       " ('ro', 5),\n",
       " ('sl', 3),\n",
       " ('cy', 3),\n",
       " ('sq', 2),\n",
       " ('tl', 2),\n",
       " ('ht', 2),\n",
       " ('jw', 1),\n",
       " ('gl', 1),\n",
       " ('hi', 1),\n",
       " ('ceb', 1),\n",
       " ('af', 1),\n",
       " ('so', 1),\n",
       " ('la', 1),\n",
       " ('ky', 1),\n",
       " ('mi', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_cnt = Counter(lang)\n",
    "sorted(lang_cnt.items(), key = lambda kv:kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Step\n",
    "##### 1. Tokenization\n",
    "##### 2. Stop word elimination\n",
    "##### 3. Stemming : 단어를 기본형으로 바꾸어준다. 복수형은 단수형으로, 과거형은 현재형으로 바꾸는 과정\n",
    "##### 4. Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_en = []\n",
    "\n",
    "for listing_id, rev_lst in reviews.items():\n",
    "    for rev in rev_lst:\n",
    "        if rev['language']=='en' :\n",
    "            comments_en.append(rev['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238228"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_en) # 238308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization package\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'do', 'anything', '!', '(', 'Oh', ',', 'no', ')']\n",
      "['I', \"can't\", 'do', 'anything', 'Oh', 'no']\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "string = \"I can't do anything!(Oh, no)\"\n",
    "print(nltk.word_tokenize(string))  # 가장 기본적인 tokenization 함수. space 단위와 구두점(punctuation)을 기준으로 토큰화\n",
    "print(nltk.regexp_tokenize(string ,\"[\\w']+\")) # 정규표현은 텍스트를 어떻게 토큰화(Tokenize) 할건지에 대해 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is im so hyun'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string2 = \"My name is IM SO HYUN\"\n",
    "string2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jin's place is a great place to stay close to PNU (about a 20 minute walk). Her family is very warm and kind, the room is nice, and the price is reasonable for the area (though maybe a little expensive if you're on a student budget). I thoroughly enjoyed my time staying in their apartment. Its a quiet neighborhood with a lot of young families, but campus and student life is also very close. I highly recommend for anyone who wants to experience 'normal' Korean every day life and areas.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jin's place is a great place to stay close to pnu (about a 20 minute walk). her family is very warm and kind, the room is nice, and the price is reasonable for the area (though maybe a little expensive if you're on a student budget). i thoroughly enjoyed my time staying in their apartment. its a quiet neighborhood with a lot of young families, but campus and student life is also very close. i highly recommend for anyone who wants to experience 'normal' korean every day life and areas.\n"
     ]
    }
   ],
   "source": [
    "comment_low = comments_en[0].lower()\n",
    "print(comment_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"jin's\",\n",
       " 'place',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'place',\n",
       " 'to',\n",
       " 'stay',\n",
       " 'close',\n",
       " 'to',\n",
       " 'pnu',\n",
       " 'about',\n",
       " 'a',\n",
       " '20',\n",
       " 'minute',\n",
       " 'walk',\n",
       " 'her',\n",
       " 'family',\n",
       " 'is',\n",
       " 'very',\n",
       " 'warm',\n",
       " 'and',\n",
       " 'kind',\n",
       " 'the',\n",
       " 'room',\n",
       " 'is',\n",
       " 'nice',\n",
       " 'and',\n",
       " 'the',\n",
       " 'price',\n",
       " 'is',\n",
       " 'reasonable',\n",
       " 'for',\n",
       " 'the',\n",
       " 'area',\n",
       " 'though',\n",
       " 'maybe',\n",
       " 'a',\n",
       " 'little',\n",
       " 'expensive',\n",
       " 'if',\n",
       " \"you're\",\n",
       " 'on',\n",
       " 'a',\n",
       " 'student',\n",
       " 'budget',\n",
       " 'i',\n",
       " 'thoroughly',\n",
       " 'enjoyed',\n",
       " 'my',\n",
       " 'time',\n",
       " 'staying',\n",
       " 'in',\n",
       " 'their',\n",
       " 'apartment',\n",
       " 'its',\n",
       " 'a',\n",
       " 'quiet',\n",
       " 'neighborhood',\n",
       " 'with',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'young',\n",
       " 'families',\n",
       " 'but',\n",
       " 'campus',\n",
       " 'and',\n",
       " 'student',\n",
       " 'life',\n",
       " 'is',\n",
       " 'also',\n",
       " 'very',\n",
       " 'close',\n",
       " 'i',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'who',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'experience',\n",
       " \"'normal'\",\n",
       " 'korean',\n",
       " 'every',\n",
       " 'day',\n",
       " 'life',\n",
       " 'and',\n",
       " 'areas']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(comment_low ,\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = []\n",
    "for i in range(len(comments_en)) : \n",
    "    comment_low = comments_en[i].lower()\n",
    "    token.extend(nltk.regexp_tokenize(comment_low ,\"[\\w']+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12532762"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"jin's\", 'place', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(token[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stop words elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'of', \"that'll\", 'few', 'too', 'hasn', 'are', 'into', \"she's\", 'yours', 'theirs', 'at', 'again', 'how', \"shan't\", 'wouldn', 'yourself', 'did', 'there', 'but', \"doesn't\", 'with', 'here', \"don't\", 'and', 'when', \"hadn't\", \"aren't\", 'under', 'what', \"haven't\", 'her', 'further', 'am', 'whom', 'd', 'mustn', 'its', 'which', 'yourselves', 'their', 'being', 'all', 'be', \"it's\", 'down', 'herself', 're', 'other', \"couldn't\", 'didn', 'was', 'by', 'any', 'she', \"isn't\", 'why', 'while', 'were', 'to', 'isn', 'against', 'up', \"you'd\", \"hasn't\", 'those', 'ours', 'his', 'about', \"mustn't\", 'have', 'above', \"weren't\", 'had', 'himself', 'will', 'just', 'he', 'these', 'has', 'not', 'who', 'them', 'shouldn', \"wasn't\", 'both', 'mightn', 'wasn', 'each', 'now', \"you'll\", 'aren', 'hadn', 'most', 'hers', 'is', 'ma', 't', 'needn', 'an', 'only', 'can', 'ourselves', \"you're\", 'that', 'very', 'been', 'we', \"mightn't\", \"you've\", 'during', 'having', 'such', 'won', 'off', 'itself', 'do', 'you', 's', 'if', 'than', 'for', 'no', 'in', 'ain', 'doing', 'because', 'where', \"won't\", 'some', 'shan', 'll', 'from', 'as', 'out', \"should've\", 'before', \"shouldn't\", 'then', 'same', 'themselves', 'below', 'over', 'the', 'nor', 've', \"wouldn't\", 'until', 'more', 'it', 'don', 'they', 'this', 'our', 'o', 'haven', 'or', 'your', 'own', 'y', \"didn't\", 'after', 'so', 'couldn', 'should', \"needn't\", 'myself', 'through', 'weren', 'my', 'between', 'm', 'once', 'i', 'a', 'him', 'me', 'doesn', 'does', 'on'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_token = [w for w in token if not w in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6698740\n",
      "5834022\n"
     ]
    }
   ],
   "source": [
    "print(len(filter_token)) #670만\n",
    "print(len(token)-len(filter_token)) #1253만-670만 = 583만이 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"jin's\", 'place', 'great', 'place', 'stay', 'close', 'pnu', '20', 'minute']\n"
     ]
    }
   ],
   "source": [
    "print(filter_token[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "p_stemmer = PorterStemmer() # Porter 알고리즘은 영어의 접미사를 제거\n",
    "l_stemmer = LancasterStemmer() # PorterStemmer와 비슷하지만 좀 더 성능이 좋다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = [p_stemmer.stem(i) for i in filter_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens_l = [l_stemmer.stem(i) for i in filter_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6698740 6698740\n"
     ]
    }
   ],
   "source": [
    "print(len(stemmed_tokens),len(stemmed_tokens_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태그\n",
    "tags_tokens = nltk.pos_tag(stemmed_tokens_l) # LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"jin's\", 'NN'), ('plac', 'NN'), ('gre', 'NN'), ('plac', 'NN'), ('stay', 'NN'), ('clos', 'VBP'), ('pnu', 'NN'), ('20', 'CD'), ('minut', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(tags_tokens[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_tokens_p = nltk.pos_tag(stemmed_tokens) # PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"jin'\", 'NN'), ('place', 'NN'), ('great', 'JJ'), ('place', 'NN'), ('stay', 'VBP'), ('close', 'RB'), ('pnu', 'JJ'), ('20', 'CD'), ('minut', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(tags_tokens_p[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 3220055),\n",
       " ('JJ', 1467843),\n",
       " ('RB', 420567),\n",
       " ('VBP', 344677),\n",
       " ('VB', 279444),\n",
       " ('IN', 167549),\n",
       " ('CD', 164540),\n",
       " ('VBD', 154575),\n",
       " ('NNS', 103067),\n",
       " ('PRP', 75431),\n",
       " ('MD', 71131),\n",
       " ('NNP', 50903),\n",
       " ('VBZ', 44216),\n",
       " ('VBN', 33779),\n",
       " ('JJS', 29706),\n",
       " ('JJR', 24447),\n",
       " ('FW', 15873),\n",
       " ('RBR', 8592),\n",
       " ('DT', 5105),\n",
       " ('RP', 4581),\n",
       " ('CC', 4129),\n",
       " ('RBS', 2025),\n",
       " ('VBG', 1648),\n",
       " ('WP', 1262),\n",
       " ('$', 972),\n",
       " ('POS', 804),\n",
       " ('WRB', 606),\n",
       " ('PRP$', 401),\n",
       " ('WDT', 267),\n",
       " ('UH', 168),\n",
       " ('EX', 152),\n",
       " (\"''\", 98),\n",
       " ('WP$', 65),\n",
       " ('TO', 30),\n",
       " ('PDT', 23),\n",
       " ('SYM', 6),\n",
       " ('NNPS', 3)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = []\n",
    "\n",
    "for a, b in tags_tokens_p :\n",
    "    pos.append(b)\n",
    "pos_cnt = Counter(pos)\n",
    "sorted(pos_cnt.items(), key = lambda kv:kv[1], reverse=True)\n",
    "\n",
    "# CC coordinating conjunction / CD cardinal digit\n",
    "# DT determiner\n",
    "# EX existential there (like: “there is” … think of it like “there exists”)\n",
    "# FW foreign word\n",
    "# IN preposition/subordinating conjunction\n",
    "### JJ adjective ‘big’/ JJR adjective, comparative ‘bigger’/ JJS adjective, superlative ‘biggest’\n",
    "# LS list marker 1)\n",
    "# MD modal could, will\n",
    "### NN noun, singular ‘desk’NNS noun plural ‘desks’/ NNP proper noun, singular ‘Harrison’/ NNPS proper noun, plural ‘Americans’\n",
    "# PDT predeterminer ‘all the kids’\n",
    "# POS possessive ending parent’s\n",
    "# PRP personal pronoun I, he, she / PRP$ possessive pronoun my, his, hers\n",
    "### RB adverb very, silently / RBR adverb, comparative better / RBS adverb, superlative best\n",
    "# RP particle give up\n",
    "# TO, to go ‘to’ the store.\n",
    "# UH interjection, errrrrrrrm\n",
    "### VB verb, base form take / VBD verb, past tense took / VBG verb, gerund present participle taking / VBN verb, past participle taken / VBP verb, sing. present, non-3d take / VBZ verb, 3rd person sing. present takes\n",
    "# WDT wh-determiner which / WP wh-pronoun who, what / WP$ possessive wh-pronoun whose / WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['univers',\n",
       " 'mom',\n",
       " 'insid',\n",
       " 'busan',\n",
       " 'busan',\n",
       " 'neighbour',\n",
       " 'amen',\n",
       " 'nini',\n",
       " 'min',\n",
       " 'busan']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [a for a,b in tags_tokens_p if b=='NNS']\n",
    "tmp[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('flies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fly', 'NN')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([lemmatizer.lemmatize('fly')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_tokens = [lemmatizer.lemmatize(i) for i in filter_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6698740"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-9f6e6853f9ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 품사 태그\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtags_tokens_lemma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \"\"\"\n\u001b[0;32m    133\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en-ptb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mprev2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 품사 태그\n",
    "tags_tokens_lemma = nltk.pos_tag(lemma_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"jin's\", 'JJ'), ('place', 'NN'), ('great', 'JJ'), ('place', 'NN'), ('stay', 'VBP'), ('close', 'RB'), ('pnu', 'JJ'), ('20', 'CD'), ('minute', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(tags_tokens_lemma[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 2759156),\n",
       " ('JJ', 1602565),\n",
       " ('RB', 645393),\n",
       " ('VBP', 402443),\n",
       " ('VB', 274807),\n",
       " ('NNS', 267553),\n",
       " ('IN', 166967),\n",
       " ('CD', 163690),\n",
       " ('PRP', 75139),\n",
       " ('MD', 69505),\n",
       " ('VBD', 64918),\n",
       " ('NNP', 50828),\n",
       " ('VBZ', 29914),\n",
       " ('JJS', 25260),\n",
       " ('JJR', 21793),\n",
       " ('VBN', 19052),\n",
       " ('DT', 13072),\n",
       " ('RBR', 9915),\n",
       " ('FW', 9550),\n",
       " ('CC', 6619),\n",
       " ('RP', 5701),\n",
       " ('VBG', 5664),\n",
       " ('WRB', 2863),\n",
       " ('RBS', 2239),\n",
       " ('WDT', 1077),\n",
       " ('WP', 944),\n",
       " ('POS', 772),\n",
       " ('$', 557),\n",
       " ('UH', 256),\n",
       " ('PRP$', 255),\n",
       " (\"''\", 76),\n",
       " ('WP$', 65),\n",
       " ('EX', 34),\n",
       " ('PDT', 32),\n",
       " ('NNPS', 26),\n",
       " ('TO', 24),\n",
       " ('SYM', 16)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lemma = []\n",
    "\n",
    "for a, b in tags_tokens_lemma :\n",
    "    pos_lemma.append(b)\n",
    "pos_cnt_lemma = Counter(pos_lemma)\n",
    "sorted(pos_cnt_lemma.items(), key = lambda kv:kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['families',\n",
       " 'areas',\n",
       " 'minutes',\n",
       " 'marts',\n",
       " 'days',\n",
       " 'tell',\n",
       " 'things',\n",
       " 'attractions',\n",
       " 'days',\n",
       " 'people']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [a for a,b in tags_tokens_lemma if b=='NNS']\n",
    "tmp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank',\n",
       " 'felt',\n",
       " 'bus',\n",
       " 'thank',\n",
       " 'hearted',\n",
       " 'felt',\n",
       " 'charm',\n",
       " 'felt',\n",
       " 'bed',\n",
       " 'host',\n",
       " 'busan',\n",
       " 'korea',\n",
       " 'let',\n",
       " 'hearted',\n",
       " 'sophie',\n",
       " 'treat',\n",
       " 'thank',\n",
       " 'bed',\n",
       " 'dryer',\n",
       " 'thank']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb = [a for a,b in tags_tokens_lemma if b=='VBD']\n",
    "verb[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# LDA package\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('fly', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('fly', 'VBP')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"i\",\"am\",\"a\",\"fly\",\"i\",\"fly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'), ('fly', 'VBP')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"i\",\"fly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 'PRP'), (\"doesn't\", 'VBD')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag([\"he\",\"doesn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
