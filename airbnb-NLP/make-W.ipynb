{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 구글드라이브 DATA에 데이터있음\n",
    "with open('no_pos_review.json') as fp:\n",
    "    review = json.load(fp)\n",
    "with open('voca_freq_dic_new.json') as fp:\n",
    "    voca_freq_dic = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# key_lst = list(review.keys())\n",
    "# sampled_key_lst = key_lst[:3000]\n",
    "# review2 = {}\n",
    "# for k in sampled_key_lst:\n",
    "#     review2[k] = review[k]\n",
    "# len(review2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voca_freq_dic2 = {} # freq로 걸러서 일부 단어만 사용\n",
    "for k,v in voca_freq_dic.items():\n",
    "    if v>99:\n",
    "        voca_freq_dic2[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4254"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voca_freq_dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(voca_freq_dic2.items(),key=lambda kv:kv[1],reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA 결과(토픽별 키워드 리스트)를 aspect key words로 갖다써보자\n",
    "# 현재 불용어처리 등 덜된상태\n",
    "tmp = {0: [('station_N', 0.075440474),\n",
    "  ('convenient_N', 0.035710342),\n",
    "  ('location_N', 0.033328794),\n",
    "  ('walk_V', 0.03065599),\n",
    "  ('subway_N', 0.026240626),\n",
    "  ('place_N', 0.023897689),\n",
    "  ('minute_N', 0.023682337),\n",
    "  ('easy_J', 0.022524938),\n",
    "  ('locate_V', 0.019995727),\n",
    "  ('store_N', 0.019815892),\n",
    "  ('close_J', 0.019692997),\n",
    "  ('close_R', 0.019388903),\n",
    "  ('away_R', 0.01791159),\n",
    "  ('bus_N', 0.017175121),\n",
    "  ('airport_N', 0.0151694175),\n",
    "  ('convenience_N', 0.014715803),\n",
    "  ('also_R', 0.014201972),\n",
    "  ('walk_N', 0.013559201),\n",
    "  ('subway_J', 0.01225483),\n",
    "  ('apartment_N', 0.01212426),\n",
    "  ('convenient_J', 0.01193456),\n",
    "  ('go_V', 0.010800422),\n",
    "  ('distance_N', 0.010039834),\n",
    "  ('find_V', 0.009765462),\n",
    "  ('subway_R', 0.009758889),\n",
    "  ('take_V', 0.009583443),\n",
    "  ('get_V', 0.009291667),\n",
    "  ('line_N', 0.009158479),\n",
    "  ('restaurant_N', 0.0081279725),\n",
    "  ('house_N', 0.007849644)],\n",
    " 1: [('good_J', 0.09984607),\n",
    "  ('nice_J', 0.0702112),\n",
    "  ('location_N', 0.065345466),\n",
    "  ('really_R', 0.06501797),\n",
    "  ('place_N', 0.053646892),\n",
    "  ('comfortable_J', 0.033898752),\n",
    "  ('clean_J', 0.03378657),\n",
    "  ('stay_N', 0.025737181),\n",
    "  ('house_N', 0.025186999),\n",
    "  ('accommodation_N', 0.016483787),\n",
    "  ('apartment_N', 0.016000627),\n",
    "  ('perfect_J', 0.015435147),\n",
    "  ('space_N', 0.01426183),\n",
    "  ('bed_N', 0.0142121855),\n",
    "  ('wifi_N', 0.013773334),\n",
    "  ('landlord_N', 0.010759912),\n",
    "  ('perfect_N', 0.010674935),\n",
    "  ('amenity_N', 0.009882116),\n",
    "  ('new_J', 0.009253663),\n",
    "  ('tidy_N', 0.008537611),\n",
    "  ('quiet_J', 0.008063444),\n",
    "  ('lovely_R', 0.007641677),\n",
    "  ('central_J', 0.0074305693),\n",
    "  ('pretty_R', 0.007232856),\n",
    "  ('neat_N', 0.007133313),\n",
    "  ('egg_N', 0.007132264),\n",
    "  ('expect_V', 0.0070267115),\n",
    "  ('photo_N', 0.0068760742),\n",
    "  ('facility_N', 0.006667349),\n",
    "  ('felt_V', 0.006483669)],\n",
    " 2: [('area_N', 0.06683145),\n",
    "  ('night_N', 0.035611134),\n",
    "  ('quiet_J', 0.030158201),\n",
    "  ('quite_R', 0.023574363),\n",
    "  ('view_N', 0.023483817),\n",
    "  ('price_N', 0.022715302),\n",
    "  ('bit_N', 0.021813327),\n",
    "  ('see_V', 0.02036446),\n",
    "  ('neighborhood_N', 0.019612567),\n",
    "  ('right_J', 0.017915798),\n",
    "  ('little_J', 0.01292769),\n",
    "  ('city_N', 0.012332531),\n",
    "  ('part_N', 0.011145581),\n",
    "  ('apartment_N', 0.010512025),\n",
    "  ('beautiful_J', 0.010445903),\n",
    "  ('locate_V', 0.010315028),\n",
    "  ('park_N', 0.009846657),\n",
    "  ('building_N', 0.009445184),\n",
    "  ('issue_N', 0.008640434),\n",
    "  ('reply_V', 0.008343354),\n",
    "  ('soon_R', 0.008054228),\n",
    "  ('hard_J', 0.008007643),\n",
    "  ('window_N', 0.007892024),\n",
    "  ('also_R', 0.0077502076),\n",
    "  ('open_J', 0.007630731),\n",
    "  ('door_N', 0.007591834),\n",
    "  ('especially_R', 0.0068424055),\n",
    "  ('get_V', 0.006388696),\n",
    "  ('house_N', 0.006340781),\n",
    "  ('perfect_V', 0.0062577077)],\n",
    " 3: [('great_J', 0.15104426),\n",
    "  ('home_N', 0.03838182),\n",
    "  ('experience_N', 0.034535814),\n",
    "  ('thank_N', 0.03342067),\n",
    "  ('stay_N', 0.022656428),\n",
    "  ('make_V', 0.01760778),\n",
    "  ('thing_N', 0.01690892),\n",
    "  ('airbnb_N', 0.015427705),\n",
    "  ('place_V', 0.014597721),\n",
    "  ('feel_N', 0.014560298),\n",
    "  ('overall_J', 0.013511427),\n",
    "  ('super_J', 0.012852805),\n",
    "  ('apartment_N', 0.0127586955),\n",
    "  ('feel_V', 0.012492993),\n",
    "  ('instruction_N', 0.011910272),\n",
    "  ('know_V', 0.009106032),\n",
    "  ('flat_J', 0.009034321),\n",
    "  ('house_N', 0.008116479),\n",
    "  ('ive_J', 0.008065549),\n",
    "  ('anything_N', 0.0076171067),\n",
    "  ('meet_N', 0.007522718),\n",
    "  ('value_N', 0.0071481066),\n",
    "  ('book_V', 0.00696656),\n",
    "  ('choice_N', 0.0067781466),\n",
    "  ('different_J', 0.0067624864),\n",
    "  ('reply_N', 0.0065802587),\n",
    "  ('never_R', 0.0061996435),\n",
    "  ('hospitality_N', 0.006097631),\n",
    "  ('jay_N', 0.0060664965),\n",
    "  ('bad_J', 0.0058132866)],\n",
    " 4: [('room_N', 0.112217285),\n",
    "  ('well_R', 0.051788304),\n",
    "  ('everything_N', 0.051343787),\n",
    "  ('clean_J', 0.050989766),\n",
    "  ('house_N', 0.025484154),\n",
    "  ('apartment_N', 0.02400923),\n",
    "  ('need_V', 0.022365913),\n",
    "  ('clean_N', 0.021655692),\n",
    "  ('small_J', 0.02147159),\n",
    "  ('comfortable_J', 0.01924563),\n",
    "  ('need_N', 0.017338376),\n",
    "  ('cozy_N', 0.016669074),\n",
    "  ('big_J', 0.014753495),\n",
    "  ('spacious_J', 0.013613145),\n",
    "  ('bathroom_N', 0.012964344),\n",
    "  ('nice_N', 0.01125928),\n",
    "  ('people_N', 0.01100092),\n",
    "  ('picture_N', 0.010688399),\n",
    "  ('provide_V', 0.010524188),\n",
    "  ('warm_J', 0.00983317),\n",
    "  ('kitchen_N', 0.009782993),\n",
    "  ('clean_V', 0.009278437),\n",
    "  ('also_R', 0.008697552),\n",
    "  ('responsive_J', 0.008031375),\n",
    "  ('towel_N', 0.007448198),\n",
    "  ('large_J', 0.0072318553),\n",
    "  ('bed_V', 0.0071571474),\n",
    "  ('equip_V', 0.0070687374),\n",
    "  ('enough_R', 0.0065676877),\n",
    "  ('live_V', 0.006433929)],\n",
    " 5: [('even_R', 0.023927504),\n",
    "  ('floor_N', 0.020947738),\n",
    "  ('get_V', 0.016879946),\n",
    "  ('little_J', 0.01670006),\n",
    "  ('problem_N', 0.016578786),\n",
    "  ('water_N', 0.016478194),\n",
    "  ('exactly_R', 0.013305269),\n",
    "  ('extremely_R', 0.0122868465),\n",
    "  ('didnt_N', 0.011868971),\n",
    "  ('use_N', 0.01179084),\n",
    "  ('machine_N', 0.011563873),\n",
    "  ('show_V', 0.011448914),\n",
    "  ('night_N', 0.011121356),\n",
    "  ('wash_V', 0.010934781),\n",
    "  ('late_J', 0.010417275),\n",
    "  ('check_V', 0.010238513),\n",
    "  ('hot_J', 0.010211995),\n",
    "  ('use_V', 0.010035234),\n",
    "  ('however_R', 0.009700446),\n",
    "  ('arrive_V', 0.009524829),\n",
    "  ('accommodate_V', 0.009023434),\n",
    "  ('early_J', 0.0089837825),\n",
    "  ('also_R', 0.008448831),\n",
    "  ('air_N', 0.008398446),\n",
    "  ('check_N', 0.008317534),\n",
    "  ('still_R', 0.00801234),\n",
    "  ('leave_V', 0.007958689),\n",
    "  ('toilet_N', 0.0075545954),\n",
    "  ('bed_N', 0.0074843178),\n",
    "  ('find_V', 0.0071834484)],\n",
    " 6: [('day_N', 0.044497136),\n",
    "  ('make_V', 0.035028186),\n",
    "  ('time_N', 0.02947009),\n",
    "  ('friend_N', 0.023036933),\n",
    "  ('much_J', 0.022923442),\n",
    "  ('stay_V', 0.020721437),\n",
    "  ('help_N', 0.020701654),\n",
    "  ('love_V', 0.020222116),\n",
    "  ('trip_N', 0.018225666),\n",
    "  ('go_V', 0.018032515),\n",
    "  ('first_J', 0.017838366),\n",
    "  ('family_N', 0.016667131),\n",
    "  ('wonderful_J', 0.01531437),\n",
    "  ('sure_J', 0.012547571),\n",
    "  ('quickly_R', 0.012029172),\n",
    "  ('first_R', 0.010693285),\n",
    "  ('im_N', 0.010558216),\n",
    "  ('welcome_V', 0.010431975),\n",
    "  ('house_N', 0.009905076),\n",
    "  ('night_N', 0.009419678),\n",
    "  ('get_V', 0.009239833),\n",
    "  ('way_N', 0.009100049),\n",
    "  ('happy_J', 0.008998546),\n",
    "  ('even_R', 0.00883439),\n",
    "  ('luggage_N', 0.008800614),\n",
    "  ('recommended_J', 0.008401327),\n",
    "  ('respond_V', 0.008349242),\n",
    "  ('second_J', 0.008234377),\n",
    "  ('able_J', 0.008000888),\n",
    "  ('chance_N', 0.007491548)]\n",
    "       ,\n",
    " 7: [('lot_N', 0.043792848),\n",
    "  ('restaurant_N', 0.040813655),\n",
    "  ('many_J', 0.03109465),\n",
    "  ('food_N', 0.029509438),\n",
    "  ('good_J', 0.025894398),\n",
    "  ('take_V', 0.024536517),\n",
    "  ('min_N', 0.024025736),\n",
    "  ('street_N', 0.021218892),\n",
    "  ('shop_N', 0.020949543),\n",
    "  ('there_N', 0.015530614),\n",
    "  ('korean_J', 0.015057178),\n",
    "  ('person_N', 0.014282671),\n",
    "  ('shop_V', 0.0127358185),\n",
    "  ('studio_N', 0.012632791),\n",
    "  ('also_R', 0.012036752),\n",
    "  ('excellent_J', 0.011713238),\n",
    "  ('transportation_N', 0.011573859),\n",
    "  ('coffee_N', 0.011001995),\n",
    "  ('fantastic_J', 0.01008252),\n",
    "  ('nearby_R', 0.010082044),\n",
    "  ('local_J', 0.009666101),\n",
    "  ('warm_N', 0.009533902),\n",
    "  ('rest_N', 0.009117627),\n",
    "  ('care_N', 0.009051847),\n",
    "  ('bar_N', 0.008858041),\n",
    "  ('university_N', 0.0079645645),\n",
    "  ('traditional_J', 0.0073084603),\n",
    "  ('life_N', 0.007216505),\n",
    "  ('house_N', 0.0067656073),\n",
    "  ('thing_N', 0.0066509987)],\n",
    " 8: [('place_N', 0.09395834),\n",
    "  ('stay_V', 0.06079642),\n",
    "  ('stay_N', 0.05158205),\n",
    "  ('time_N', 0.04721144),\n",
    "  ('recommend_V', 0.041916873),\n",
    "  ('come_V', 0.03651728),\n",
    "  ('definitely_R', 0.036476232),\n",
    "  ('next_J', 0.03116234),\n",
    "  ('back_R', 0.029059581),\n",
    "  ('go_V', 0.023483075),\n",
    "  ('thanks_N', 0.019712204),\n",
    "  ('want_V', 0.019274745),\n",
    "  ('visit_N', 0.01906872),\n",
    "  ('highly_R', 0.017565712),\n",
    "  ('enjoy_V', 0.01718251),\n",
    "  ('best_J', 0.010691537),\n",
    "  ('amaze_V', 0.010670577),\n",
    "  ('stay_J', 0.010333447),\n",
    "  ('house_N', 0.010283317),\n",
    "  ('apartment_N', 0.009439452),\n",
    "  ('stop_N', 0.009254608),\n",
    "  ('look_V', 0.008955529),\n",
    "  ('guesthouse_N', 0.0072605656),\n",
    "  ('anyone_N', 0.007109243),\n",
    "  ('youre_N', 0.0068305926),\n",
    "  ('trip_N', 0.006618274),\n",
    "  ('book_N', 0.006315217),\n",
    "  ('visit_V', 0.0062876446),\n",
    "  ('use_N', 0.0058828676),\n",
    "  ('week_N', 0.0058368403)],\n",
    " 9: [('host_N', 0.0983368),\n",
    "  ('kind_N', 0.034277067),\n",
    "  ('helpful_J', 0.025665222),\n",
    "  ('help_V', 0.022550533),\n",
    "  ('give_V', 0.020565689),\n",
    "  ('also_R', 0.020454295),\n",
    "  ('nice_J', 0.020182502),\n",
    "  ('really_R', 0.01883619),\n",
    "  ('provide_V', 0.018348224),\n",
    "  ('question_N', 0.017101107),\n",
    "  ('friendly_R', 0.01532856),\n",
    "  ('always_R', 0.014121552),\n",
    "  ('friendly_J', 0.013314891),\n",
    "  ('helpful_N', 0.012839154),\n",
    "  ('guest_N', 0.012607242),\n",
    "  ('owner_N', 0.011304215),\n",
    "  ('breakfast_N', 0.010966653),\n",
    "  ('even_R', 0.009854197),\n",
    "  ('recommend_J', 0.009247552),\n",
    "  ('quick_J', 0.009128347),\n",
    "  ('communication_N', 0.008465741),\n",
    "  ('meet_V', 0.007926063),\n",
    "  ('information_N', 0.007655206),\n",
    "  ('get_V', 0.007644172),\n",
    "  ('direction_N', 0.0075688995),\n",
    "  ('easy_J', 0.0075597554),\n",
    "  ('need_V', 0.006919648),\n",
    "  ('shopping_N', 0.0067448863),\n",
    "  ('free_J', 0.006327245),\n",
    "  ('staff_N', 0.005882458)]\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = {}\n",
    "for k in range(len(tmp)):\n",
    "    seed_words[k] = []\n",
    "    for idx,tpl in enumerate(tmp[k]):\n",
    "        seed_words[k].append( (tpl[0][:-2],tpl[1]) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aspect_keywords = []\n",
    "for i,k_words in seed_words.items():\n",
    "    keys,b = zip(*k_words)\n",
    "    aspect_keywords.append(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_aspect_terms(aspects, vocab_dict):\n",
    "    aspect_terms = []\n",
    "    w_notfound = []\n",
    "    \n",
    "    for aspect_kws in aspects:\n",
    "        aspect = []\n",
    "        for w in aspect_kws:\n",
    "            if w in vocab_dict:\n",
    "                aspect.append(w)\n",
    "            else:\n",
    "                w_notfound.append(w)\n",
    "        aspect_terms.append(aspect)\n",
    "    #We are only using one hotel review file, as we keep inceasing the number of files words not found will decrease.\n",
    "    # print \"Words not found in vocab:\", ' '.join(w_notfound)\n",
    "    return aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chi_sq(a,b,c,d):\n",
    "    c1 = a\n",
    "    c2 = b - a\n",
    "    c3 = c - a\n",
    "    c4 = d - b - c + a\n",
    "    nc =  d\n",
    "    return nc * (c1*c4 - c2*c3) * (c1*c4 - c2*c3)/((c1+c3) * (c2+c4) * (c1+c2) * (c3+c4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chi_sq_mat():\n",
    "    global aspect_words, aspect_sent, num_words\n",
    "    asp_rank = np.zeros(aspect_words.shape)\n",
    "    for i in range(len(aspect_terms)):\n",
    "        for j in range(len(vocab)):\n",
    "            asp_rank[i][j] = chi_sq(aspect_words[i][j], num_words[j], aspect_sent[i], len(sent))\n",
    "    return asp_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aspect_terms = get_aspect_terms(aspect_keywords,voca_freq_dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aspect_segmentaion(review_dic,voca_freq_dic):\n",
    "    #INPUT\n",
    "    review_sent = [] # 문장, 단어단위로 토크나이징된 리뷰 담은 리스트\n",
    "    for rev_lst in review_dic.values():\n",
    "        for rev in rev_lst:\n",
    "            review_sent.append(rev[1])\n",
    "    \n",
    "    all_ratings = []\n",
    "    for rev_lst in review_dic.values():\n",
    "        for rev in rev_lst:\n",
    "            all_ratings.append(rev[0])\n",
    "\n",
    "    #selection threshold\n",
    "    p = 5\n",
    "    \n",
    "    #Iterations \n",
    "    # I = 10\n",
    "    I = 1\n",
    "\n",
    "#     #Create Vocabulary\n",
    "#     #review_sent, review_actual, only_sent = parse_to_sentence(reviews)\n",
    "#     #vocab, #vocab_dict = create_vocab(only_sent)\n",
    "\n",
    "    vocab = list(voca_freq_dic.keys())\n",
    "    #Assign a number corresponding to each word. Makes counting easier.\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab)))) # word를 key 로 word의 인덱스를 value로  \n",
    "\n",
    "    #Aspect Keywords\n",
    "    aspect_terms = get_aspect_terms(aspect_keywords,voca_freq_dic)\n",
    "\n",
    "#     label_text = \n",
    "    # print aspect_terms\n",
    "\n",
    "    #ALGORITHM\n",
    "    review_labels = []\n",
    "    k = len(aspect_terms) # k: 토픽 개수 10\n",
    "    v = len(vocab) # v: 단어 개수 18923\n",
    "    \n",
    "    aspect_words = np.zeros((k,v))\n",
    "    aspect_sent = np.zeros(k)\n",
    "    num_words = np.zeros(v)\n",
    "    #-----------------------------------------------------\n",
    "    for _ in range(I):\n",
    "        # print(len(review_sent)) 리뷰개수 595387\n",
    "        for r in review_sent:\n",
    "            labels = []\n",
    "            for s in r:\n",
    "                count = np.zeros(len(aspect_terms)) # 한 문장에 대해 aspect keyword가 몇개씩있는지 카운트\n",
    "                \n",
    "                for i,a in enumerate(aspect_terms):\n",
    "                    for w in s:\n",
    "                        if w in vocab_dict:\n",
    "                            num_words[vocab_dict[w]] += 1\n",
    "                            if w in a:\n",
    "                                count[i] += 1\n",
    "\n",
    "                if max(count) > 0:\n",
    "                    la = np.where(np.max(count) == count)[0].tolist() # count 중 max인 aspect들의  인덱스 뽑아 리스트로\n",
    "                    \n",
    "                    labels.append(la)\n",
    "                    for idx in la:\n",
    "                        aspect_sent[idx] += 1\n",
    "                        for w in s:\n",
    "                            if w in vocab_dict:\n",
    "                                aspect_words[idx][vocab_dict[w]] += 1\n",
    "                else:\n",
    "                    labels.append([])\n",
    "                    \n",
    "            review_labels.append(labels)\n",
    "        \n",
    "#             aspect_w_rank = chi_sq_mat()\n",
    "#             new_labels = []\n",
    "#             for na in aspect_w_rank:\n",
    "#                 x = np.argsort(na)[::-1][:p]\n",
    "#                 new_labels.append(x)\n",
    "#                 for k,v in vocab_dict.items():\n",
    "#                     if vocab_dict[k] in x:\n",
    "#                         print(k)\n",
    "#             sys.exit()\n",
    "            \n",
    "    return review_labels,review_sent\n",
    "\n",
    "#     ratings_sentiment = []\n",
    "#     for r in review_actual:\n",
    "#         sentiment = []\n",
    "#         #aspect ratings based on sentiment\n",
    "#         for s in r:\n",
    "#             ss = sid.polarity_scores(s)\n",
    "#             sentiment.append(ss['compound'])\n",
    "#         ratings_sentiment.append(sentiment)\n",
    "\n",
    "#     #Aspect Ratings Per Review\n",
    "#     aspect_ratings = []\n",
    "#     for i,r in enumerate(review_labels):\n",
    "#         rating = np.zeros(7)\n",
    "#         count = np.zeros(7)\n",
    "#         rs = ratings_sentiment[i] \n",
    "#         for j,l in enumerate(r):\n",
    "#             for k in range(7):\n",
    "#                 if k in l:\n",
    "#                     rating[k] += rs[j]\n",
    "#             for k in range(7):\n",
    "#                 if count[k] != 0:\n",
    "#                     rating[k] /= count[k]\n",
    "#         #Map from -[-1,1] to [1,5]\n",
    "#         for k in range(7):\n",
    "#             if rating[k] != 0:\n",
    "#                 rating[k] = int(round((rating[k]+1)*5/2))\n",
    "#         aspect_ratings.append(rating)\n",
    "#     return aspect_ratings, all_ratings\n",
    "\n",
    "    # n = 0\n",
    "    # print review_actual[n], '\\n', review_labels[n]\n",
    "    # print ratings_sentiment[n], '\\n', aspect_ratings[n]\n",
    "    # print len(all_ratings), len(reviews), all_ratings[0]\n",
    "    # sys.exit()\n",
    "    # return aspect_ratings\n",
    "\n",
    "    # print sent[5:9], labels[5:9]\n",
    "    # print zip(actual_sent, labels)[:10]\n",
    "    # print zip(actual_sent, sentiment)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omit_word = []\n",
    "# for k,rev_lst in review.items():\n",
    "#     for rev in rev_lst:\n",
    "#         for i,sent in enumerate(rev[1]):\n",
    "#             sent_tmp =[]\n",
    "#             for word in sent:\n",
    "#                 if word in voca_freq_dic2:\n",
    "#                     sent_tmp.append(word)\n",
    "#                 else:\n",
    "#                     omit_word.append(word)\n",
    "#             rev[1][i] = sent_tmp\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 32s, sys: 1.98 s, total: 2min 34s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "review_labels,review_sents = aspect_segmentaion(review,voca_freq_dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# review_labels_np = []\n",
    "# for rev in review_labels:\n",
    "#     rev_np = []\n",
    "#     for sent in rev:\n",
    "#         rev_np.append(np.array(sent))\n",
    "#     review_labels_np.append(np.array(rev_np))\n",
    "# review_labels_np = np.array(review_labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([6]), array([4, 5, 6, 8]), array([0, 3, 6, 8])], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review_labels_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['room',\n",
       "  'spacious',\n",
       "  'spacious',\n",
       "  'bedroom',\n",
       "  'see',\n",
       "  'windowwhen',\n",
       "  'go',\n",
       "  'caught',\n",
       "  'typhoon',\n",
       "  'fortunately',\n",
       "  'room',\n",
       "  'occupy',\n",
       "  'doubledecked',\n",
       "  'windowsthe',\n",
       "  'room',\n",
       "  'modern',\n",
       "  'technologicaloverall',\n",
       "  'pretty',\n",
       "  'good']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('review_labels.dat','wb') as fp:\n",
    "#     review_labels.dump(fp)\n",
    "# with open('review_sents.dat','wb') as fp:\n",
    "#     review_sents.dump(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('review_labels.lst','rb') as fp:\n",
    "#     review_labels = np.load(fp)\n",
    "# with open('review_sents.lst','rb') as fp:\n",
    "#     review_sents = np.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_cnt = 0\n",
    "for rev_lst in review.values():\n",
    "    review_cnt += len(rev_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25327762980"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = review_cnt # 리뷰수\n",
    "K = len(aspect_terms) # aspect 수\n",
    "N = len(voca_freq_dic2) # 단어수\n",
    "D*K*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = np.ndarray((D,K,N),dtype = 'int16') # 메모리에 다 안올라감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = {} # k로 나눠 딕셔너리 만들자\n",
    "for i in range(K):\n",
    "    W['w_'+str(i)] = np.ndarray((D,N),dtype = 'int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for d,rev in enumerate(review_labels):\n",
    "#     for sent_idx,sent_labels in enumerate(rev):\n",
    "#         for label in sent_labels:\n",
    "#             review_sents[d][sent_idx]\n",
    "#             W[d][label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595387"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiprocessing 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kth_count(sent_labels, sents,k,freq_dic_kth):\n",
    "    for idx,label in enumerate(sent_labels):\n",
    "        if k in label: # kth aspect이면\n",
    "            for word in sents[idx]:\n",
    "                if word not in freq_dic_kth:\n",
    "                    continue\n",
    "                freq_dic_kth[word] += 1\n",
    "        \n",
    "\n",
    "def make_kth_mat(k):\n",
    "    global W,D # word freq 담을 전역변수 W. kth aspect 의 freq은 w_k (np.array)에 담는다/ 리뷰개수 D\n",
    "    for d in tqdm.tqdm(range(D)):# 리뷰개수만큼 돌면서\n",
    "        freq_dic_kth = dict.fromkeys(voca_freq_dic2.keys(),0) # kth freq dic 초기화\n",
    "        kth_count(review_labels[d],review_sents[d],k,freq_dic_kth) # freq_dic_kth 을 완성\n",
    "        words,freqs = zip(*sorted(freq_dic_kth.items(),key = lambda kv:kv[0])) # freq_dic_kth를 사전순으로 정렬해 단어와 freq를 각각 뽑는다\n",
    "        W['w_'+str(k)][d] = np.array(freqs,dtype='int16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5505/595387 [00:26<59:27, 165.36it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-9b7d9348c92f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_kth_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-5fccaa701c2c>\u001b[0m in \u001b[0;36mmake_kth_mat\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mkth_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreview_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreq_dic_kth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# freq_dic_kth 을 완성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq_dic_kth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# freq_dic_kth를 사전순으로 정렬해 단어와 freq를 각각 뽑는다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "make_kth_mat(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
