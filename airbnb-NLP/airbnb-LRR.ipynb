{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chankoo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5\n",
    "K = 6\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [5,5,4,3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    W['w_'+str(k)] = np.random.randint(0,high = 5,size=(D,N) ,dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = np.zeros((D,K,N),dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    for d in range(D): \n",
    "        W2[d][k] = W['w_'+str(k)][d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.70752466,  0.03806972,  0.02335534, -0.07184985, -0.21061131,\n",
       "       -0.31549415], dtype=float32)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_0 = np.array(np.random.rand(K) * 2.0 - np.ones((K)),dtype='float32')\n",
    "mu_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_0 = np.array(np.identity(K,dtype='float32'))\n",
    "sigma_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24938298,  0.88092023,  0.96387932,  1.19321081, -0.1043185 ,\n",
       "         1.03167607],\n",
       "       [-1.36069066,  1.27536926,  0.25290939,  0.51437761, -0.013455  ,\n",
       "        -0.91282985],\n",
       "       [-0.95626935, -0.10907779, -0.26746498,  1.16686273, -0.99313553,\n",
       "         0.2258339 ],\n",
       "       [ 2.29420473, -1.32935291, -0.41357428,  1.27115083,  0.6426769 ,\n",
       "         0.54380792],\n",
       "       [-0.44568435, -0.39624808, -0.71660223,  1.67179923, -1.01756768,\n",
       "        -0.7561521 ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_hat_0 = np.random.normal(loc = mu_0, scale= sigma_0.diagonal(),size = (D,K) )\n",
    "alpha_hat_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0144656 , 0.04479412, 0.0486687 , 0.06121348, 0.01672389,\n",
       "        0.0520827 ],\n",
       "       [0.00476104, 0.06645506, 0.02390448, 0.031048  , 0.01831466,\n",
       "        0.00745084],\n",
       "       [0.00713411, 0.01664448, 0.01420639, 0.05962169, 0.00687589,\n",
       "        0.02326594],\n",
       "       [0.18407834, 0.0049126 , 0.01227522, 0.06617532, 0.03529826,\n",
       "        0.03197533],\n",
       "       [0.01188732, 0.01248975, 0.00906621, 0.098786  , 0.00670993,\n",
       "        0.00871464]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_0 = np.exp(alpha_hat_0)/np.sum(np.exp(alpha_hat_0))\n",
    "alpha_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02548366,  0.08036695, -0.08206261,  0.08121232,  0.06949994,\n",
       "         0.09651957, -0.05386621,  0.00248668,  0.05271601, -0.00615398],\n",
       "       [-0.00534419,  0.07079478, -0.06157328,  0.0640165 , -0.05443381,\n",
       "         0.01562076, -0.02882842, -0.09236123,  0.03597522, -0.05940575],\n",
       "       [-0.03133282, -0.0516488 , -0.06427151,  0.03898093, -0.02691578,\n",
       "        -0.0241933 , -0.08954012, -0.08698585,  0.02425431,  0.06478388],\n",
       "       [ 0.00623898, -0.05383178, -0.08017184, -0.02157001,  0.06635117,\n",
       "        -0.08926516, -0.03293576,  0.08168465,  0.09177446,  0.09616955],\n",
       "       [-0.09591533,  0.02302901,  0.08371054,  0.07855335, -0.01681843,\n",
       "         0.06564218, -0.05953276,  0.06019444, -0.0639237 , -0.04285208],\n",
       "       [-0.05071892,  0.0355148 , -0.07885524, -0.02176891, -0.03722655,\n",
       "         0.06862206,  0.09413073,  0.01144626,  0.04068504,  0.01559115]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86911262, 2.51133475, 0.79262366, 0.83292179, 1.3835091 ,\n",
       "        1.72245512],\n",
       "       [1.56970731, 1.28077938, 0.53736761, 1.05700677, 1.67815938,\n",
       "        1.12308689],\n",
       "       [0.85739531, 2.42023864, 0.69253216, 1.61605403, 1.38279465,\n",
       "        1.37056831],\n",
       "       [1.72155581, 3.5254284 , 0.53955592, 1.33774667, 1.32116659,\n",
       "        1.49361631],\n",
       "       [1.23771014, 2.17791606, 0.80838737, 1.46112601, 1.41067701,\n",
       "        1.03103525]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_0 =  model.aspect_rating_T[0]\n",
    "S_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [5, 5, 4, 3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_0 = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_inv_0 = np.linalg.inv(np.identity(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[-0.25938296  0.87092024  0.95387936  1.1832108  -0.1143185   1.0216761 ]\n",
      " [-1.3706907   1.2653693   0.24290939  0.5043776  -0.023455   -0.92282987]\n",
      " [-0.9662693  -0.11907779 -0.277465    1.1568627  -1.0031356   0.2158339 ]\n",
      " [ 2.2842047  -1.3393528  -0.42357427  1.2611508   0.6326769   0.53380793]\n",
      " [-0.45568433 -0.40624806 -0.7266022   1.6617992  -1.0275676  -0.7661521 ]]\n",
      "20 [[-0.45938277  0.67092043  0.75387955  0.98321104 -0.3143185   0.82167625]\n",
      " [-1.5706905   1.0653695   0.04290935  0.3043778  -0.22345503 -1.1228297 ]\n",
      " [-1.1662692  -0.3190778  -0.4774648   0.9568629  -1.2031354   0.01583387]\n",
      " [ 2.084205   -1.5393527  -0.6235741   1.061151    0.4326771   0.33380812]\n",
      " [-0.6556842  -0.6062479  -0.926602    1.4617994  -1.2275674  -0.9661519 ]]\n",
      "40 [[-0.6593826   0.47092062  0.55387974  0.78321123 -0.51431835  0.62167645]\n",
      " [-1.7706903   0.8653697  -0.15709065  0.10437778 -0.42345488 -1.3228295 ]\n",
      " [-1.366269   -0.5190776  -0.6774646   0.7568631  -1.4031352  -0.18416613]\n",
      " [ 1.8842051  -1.7393525  -0.8235739   0.8611512   0.23267725  0.13380814]\n",
      " [-0.855684   -0.8062477  -1.1266018   1.2617996  -1.4275672  -1.1661518 ]]\n",
      "60 [[-0.8593824   0.2709208   0.35387993  0.5832114  -0.71431816  0.42167664]\n",
      " [-1.9706901   0.66536987 -0.3570906  -0.09562219 -0.6234547  -1.5228293 ]\n",
      " [-1.5662688  -0.7190774  -0.8774644   0.5568633  -1.603135   -0.38416603]\n",
      " [ 1.6842053  -1.9393523  -1.0235738   0.6611514   0.03267722 -0.06619184]\n",
      " [-1.0556839  -1.0062475  -1.3266016   1.0617998  -1.627567   -1.3661516 ]]\n",
      "80 [[-1.0593822   0.07092078  0.15387997  0.3832116  -0.91431797  0.22167678]\n",
      " [-2.17069     0.46537006 -0.5570904  -0.2956222  -0.8234545  -1.7228291 ]\n",
      " [-1.7662686  -0.9190772  -1.0774642   0.3568635  -1.8031348  -0.5841659 ]\n",
      " [ 1.4842055  -2.139352   -1.2235736   0.4611516  -0.16732278 -0.26619187]\n",
      " [-1.2556837  -1.2062473  -1.5266014   0.86179996 -1.8275669  -1.5661514 ]]\n",
      "100 [[-1.259382   -0.1290792  -0.04612002  0.1832117  -1.1143178   0.02167675]\n",
      " [-2.3706899   0.26537025 -0.7570902  -0.495622   -1.0234543  -1.9228289 ]\n",
      " [-1.9662684  -1.1190771  -1.277464    0.15686354 -2.0031347  -0.7841657 ]\n",
      " [ 1.2842057  -2.339352   -1.4235734   0.2611518  -0.3673227  -0.46619168]\n",
      " [-1.4556835  -1.4062471  -1.7266012   0.66180015 -2.0275667  -1.7661512 ]]\n",
      "120 [[-1.4593818  -0.32907918 -0.24612007 -0.01678831 -1.3143176  -0.17832325]\n",
      " [-2.5706897   0.0653702  -0.95709    -0.69562185 -1.2234541  -2.1228287 ]\n",
      " [-2.1662683  -1.3190769  -1.4774638  -0.04313646 -2.2031345  -0.9841655 ]\n",
      " [ 1.0842059  -2.5393517  -1.6235732   0.06115175 -0.56732255 -0.6661915 ]\n",
      " [-1.6556833  -1.606247   -1.926601    0.46180034 -2.2275665  -1.966151  ]]\n",
      "140 [[-1.6593816  -0.529079   -0.44611987 -0.21678834 -1.5143174  -0.37832317]\n",
      " [-2.7706895  -0.13462977 -1.1570898  -0.89562166 -1.4234539  -2.3228285 ]\n",
      " [-2.3662682  -1.5190767  -1.6774637  -0.2431365  -2.4031343  -1.1841654 ]\n",
      " [ 0.88420606 -2.7393515  -1.823573   -0.13884823 -0.76732236 -0.8661913 ]\n",
      " [-1.8556831  -1.8062468  -2.126601    0.26180053 -2.4275663  -2.1661508 ]]\n",
      "160 [[-1.8593814  -0.7290788  -0.6461197  -0.4167882  -1.7143172  -0.578323  ]\n",
      " [-2.9706893  -0.33462974 -1.3570896  -1.0956215  -1.6234537  -2.5228283 ]\n",
      " [-2.566268   -1.7190765  -1.8774635  -0.4431363  -2.6031342  -1.3841652 ]\n",
      " [ 0.68420625 -2.9393513  -2.023573   -0.3388482  -0.9673222  -1.0661912 ]\n",
      " [-2.055683   -2.0062466  -2.3266008   0.06180049 -2.627566   -2.3661506 ]]\n",
      "180 [[-2.0593812  -0.92907864 -0.8461195  -0.616788   -1.914317   -0.7783228 ]\n",
      " [-3.170689   -0.5346296  -1.5570894  -1.2956213  -1.8234535  -2.7228281 ]\n",
      " [-2.7662678  -1.9190763  -2.0774634  -0.64313614 -2.803134   -1.584165  ]\n",
      " [ 0.48420644 -3.1393511  -2.2235727  -0.53884804 -1.167322   -1.266191  ]\n",
      " [-2.2556827  -2.2062464  -2.5266006  -0.1381995  -2.827566   -2.5661504 ]]\n",
      "200 [[-2.259381   -1.1290785  -1.0461193  -0.81678784 -2.114317   -0.9783226 ]\n",
      " [-3.370689   -0.7346294  -1.7570893  -1.4956211  -2.0234535  -2.922828  ]\n",
      " [-2.9662676  -2.1190763  -2.2774632  -0.84313595 -3.0031338  -1.7841648 ]\n",
      " [ 0.28420663 -3.339351   -2.4235725  -0.73884785 -1.3673218  -1.4661908 ]\n",
      " [-2.4556825  -2.4062462  -2.7266004  -0.33819947 -3.0275657  -2.7661502 ]]\n"
     ]
    }
   ],
   "source": [
    "mu_data = mu_0\n",
    "sigma_data = sigma_0\n",
    "sigma_inv_data = np.linalg.inv(sigma_0)\n",
    "delta_data = delta_0\n",
    "s_data = S_0\n",
    "\n",
    "rating_data = [5,5,4,3,0]\n",
    " \n",
    "alpha_hat_data = alpha_hat_0\n",
    "\n",
    "alpha_hat = tf.Variable(alpha_hat_0, name = 'aspect_weight_hat',dtype=tf.float32)\n",
    "# mu = tf.placeholder(tf.float32,shape=(K),name='mu')\n",
    "# sigma_inv = tf.placeholder(tf.float32,shape=(K,K),name='sigma_inv')\n",
    "# delta = tf.placeholder(tf.float32,name='delta')\n",
    "# s = tf.placeholder(tf.float32,shape=(D,K),name = 'aspect_rating')\n",
    "# preceding = tf.placeholder(tf.float32,shape=(D,K),name='precending')\n",
    "\n",
    "mu = tf.placeholder_with_default(mu_data,shape=(K),name='mu')\n",
    "sigma_inv = tf.placeholder_with_default(sigma_inv_data,shape=(K,K),name='sigma_inv')\n",
    "delta = tf.placeholder_with_default(delta_data,shape=None,name='delta')\n",
    "s = tf.placeholder_with_default(s_data,shape=(D,K),name = 'aspect_rating')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rating = tf.placeholder(tf.int16,shape=(D))\n",
    " \n",
    "    \n",
    "preceding_data = np.zeros((D,K),dtype='float32')\n",
    "for d in range(D):\n",
    "    preceding_data[d] = (((rating_data - np.matmul(alpha_hat_data,s_data.T).diagonal())[d]*s_data[d]) )\n",
    "\n",
    "preceding = tf.placeholder_with_default(preceding_data,shape=(D,K),name='precending')\n",
    "loss =  preceding/tf.square(delta) + tf.matmul((alpha_hat-mu),sigma_inv)\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "#         sess.run([alpha_hat,train] # feed_dict 넣으면 에러 왜지\n",
    "#                  , feed_dict={preceding: preceding_data,delta:delta_data,alpha_hat:alpha_hat_data,sigma_inv:sigma_inv_data,mu:mu_data})\n",
    "        res = sess.run([alpha_hat,train])\n",
    "        if step % 20 == 0:\n",
    "            print(step, sess.run(alpha_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRR_Model:\n",
    "    global W2\n",
    "    W = W2\n",
    "    SCORE_SQUARE = False\n",
    "    \n",
    "    K = None # num of aspects\n",
    "    D = None # num of documents\n",
    "    N = None # num of words\n",
    "    \n",
    "    T = 1000 # max iteration\n",
    "    \n",
    "    mu_T = [0 for _ in range(T)]\n",
    "    sigma_T = [0 for _ in range(T)]\n",
    "    sigma_inv_T = [0 for _ in range(T)]\n",
    "    delta_T = [0 for _ in range(T)]\n",
    "    beta_T = [0 for _ in range(T)]\n",
    "    \n",
    "    aspect_weight_T = [0 for _ in range(T)] # alpha\n",
    "    alpha_hat_T = [0 for _ in range(T)]\n",
    "    \n",
    "    aspect_rating_T = [0 for _ in range(T)] # S\n",
    "    \n",
    "    ratings = None\n",
    "    \n",
    "    def __init__(self,k,d,n,ratings:list):\n",
    "        self.K = k\n",
    "        self.D = d\n",
    "        self.N = n\n",
    "        self.ratings = np.array(ratings,dtype='float64')\n",
    "        \n",
    "        self.mu_T[0] = np.array(np.random.rand(self.K) * 2.0 - np.ones(self.K) ,dtype='float64')\n",
    "        self.sigma_T[0] = np.array(np.identity(self.K),dtype='float64')\n",
    "        self.sigma_inv_T[0] = np.linalg.inv(self.sigma_T[0])\n",
    "        \n",
    "        self.beta_T[0] = (2*np.random.rand(self.K, self.N)-1.0)/10\n",
    "        self.delta_T[0] = np.ones(1,dtype='float64')\n",
    "        \n",
    "        self.alpha_hat_T[0] = np.random.normal(loc = self.mu_T[0], scale= self.sigma_T[0].diagonal(),size=(D,K) )\n",
    "        self.aspect_weight_T[0] = np.exp(self.alpha_hat_T[0])/np.sum(np.exp(self.alpha_hat_T[0]))\n",
    "        \n",
    "    \n",
    "#     def calc_covariance(self,vct):\n",
    "#         '''\n",
    "#         :param vct:\n",
    "#         :return:\n",
    "\n",
    "#         double sum = 0, s;\n",
    "# \t\tfor(int i=0; i<m_k; i++){\n",
    "# \t\t\ts = 0;\n",
    "# \t\t\tfor(int j=0; j<m_k; j++)\n",
    "# \t\t\t\ts += vct[j] * m_sigma_inv[j][i];\n",
    "# \t\t\tsum += s * vct[i];\n",
    "# \t\t}\n",
    "# \t\treturn sum;\n",
    "\n",
    "#         '''\n",
    "\n",
    "#     def calc_det(self):\n",
    "#         return np.linalg.det(self.sigma)\n",
    "\n",
    "#     def calc_sigma_inv(self,scale):\n",
    "#         self.sigma_inv = np.linalg.inv(self.sigma) * scale\n",
    "\n",
    "    def save(self,file_name:str):\n",
    "        print(os.path.join(os.getcwd(), os.path.join('model', file_name)))\n",
    "        with open(os.path.join(os.getcwd(), os.path.join('model', file_name)), 'wb') as fp:\n",
    "            pickle.dump(self,fp)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls,file_name:str):\n",
    "        try:\n",
    "            with open(os.path.join(os.getcwd(),os.path.join('model',file_name)),'rb') as fp:\n",
    "                return pickle.load(fp)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def fill_aspect_rating(self,t):\n",
    "        aspect_rating = np.zeros((D,K))\n",
    "        if self.SCORE_SQUARE==True:\n",
    "            for d in range(D):\n",
    "                aspect_rating[d] = 0.5*np.sum(np.multiply(self.beta_T[t],self.W[d]),axis=1)#to avoid negative rating \n",
    "            self.aspect_rating_T[t] = aspect_rating\n",
    "        else:\n",
    "            for d in range(D):\n",
    "                aspect_rating[d] = np.exp(np.sum(np.multiply(self.beta_T[t],self.W[d]),axis=1))#to avoid negative rating \n",
    "            self.aspect_rating_T[t] = aspect_rating\n",
    "            \n",
    "    def infer_aspect_weight(self):\n",
    "\n",
    "        alpha_hat = tf.Variable(self.alpha_hat_T[0], name = 'aspect_weight_hat',dtype=tf.float64)\n",
    "\n",
    "        mu = tf.placeholder_with_default(self.mu_T[0],shape=(K),name='mu')\n",
    "        sigma_inv = tf.placeholder_with_default(self.sigma_inv_T[0],shape=(K,K),name='sigma_inv')\n",
    "        delta = tf.placeholder_with_default(self.delta_T[0],shape=None,name='delta')\n",
    "        s = tf.placeholder_with_default(self.aspect_rating_T[0],shape=(D,K),name = 'aspect_rating')\n",
    "        ratings = tf.placeholder_with_default(self.ratings,shape=(D),name = 'ratings')\n",
    "\n",
    "#         preceding_tmp = np.zeros((D,K),dtype='float32')\n",
    "#         for d in range(D):\n",
    "#             preceding_tmp[d] = (((self.ratings - np.matmul(self.alpha_hat_T[0],self.aspect_rating_T[0].T).diagonal())[d]*self.aspect_rating_T[0][d]) )\n",
    "\n",
    "#         preceding = tf.placeholder_with_default(preceding_tmp,shape=(D,K),name='precending')\n",
    "        \n",
    "        loss = tf.square( ratings - tf.linalg.diag_part(tf.matmul(alpha_hat,tf.transpose(s)) ))/(2*tf.square(delta)) + 0.5*tf.matmul(tf.matmul(tf.subtract(alpha_hat,mu), sigma_inv),tf.transpose(tf.subtract(alpha_hat,mu)))\n",
    "\n",
    "#         grad =  preceding/tf.square(delta) + tf.matmul((alpha_hat-mu),sigma_inv)\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "        train = optimizer.minimize(loss)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for step in range(201):\n",
    "                res = sess.run([alpha_hat,train])\n",
    "                if step % 20 == 0:\n",
    "                    print(step,':')\n",
    "                    print('loss:',sess.run(loss),'\\n') \n",
    "                    print('alpha_hat:',sess.run(alpha_hat),'\\n')\n",
    "        print('result:',res[0])\n",
    "\n",
    "    \n",
    "    def e_step():\n",
    "        #infer_aspect_weight()\n",
    "        pass\n",
    "    def m_step():\n",
    "        pass\n",
    "    \n",
    "    def em_est(converge:float):\n",
    "#         e_step()\n",
    "#         m_step()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LRR_Model(6,5,10,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chankoo\\Desktop\\GitHub\\BOAZ-projects\\airbnb-NLP\\model\\test.model\n"
     ]
    }
   ],
   "source": [
    "model.save('test.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LRR_Model.load('test.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fill_aspect_rating(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39832176, 0.56760971, 0.80171447, 1.4554806 , 1.02222094,\n",
       "        0.80951094],\n",
       "       [0.75669864, 0.7599482 , 1.21590412, 1.10563484, 1.12902233,\n",
       "        0.68276736],\n",
       "       [0.60445405, 0.47784935, 0.99632811, 0.8517048 , 0.92164397,\n",
       "        0.54869252],\n",
       "       [0.57487901, 0.95427958, 0.42680908, 1.29230463, 0.91091262,\n",
       "        1.36451713],\n",
       "       [0.89017478, 0.59392093, 0.67035177, 1.72723245, 1.34795057,\n",
       "        0.52122495]])"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.aspect_rating_T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :\n",
      "loss: [[ 7.26592326  1.09964463  3.16368401  8.04013833  0.24624355]\n",
      " [ 6.28793394  3.37423178  4.07109154 10.81907101  0.44611301]\n",
      " [ 5.46697817  1.18609639  7.55853608 10.06087574 -0.37024145]\n",
      " [ 5.83204693  3.42269029  5.54949018 11.94889954  0.08954825]\n",
      " [ 5.94724896  0.95882911  3.02746979  7.99864506  1.4211611 ]] \n",
      "\n",
      "alpha_hat: [[ 0.89418232  0.38143726  0.73231246 -0.13649226 -0.80850131  1.69898906]\n",
      " [ 1.42686552  1.95782716  1.49266372  1.29522187 -0.09259236  0.92702311]\n",
      " [ 2.20568328  0.89520648 -0.671558    2.15147759 -2.08430444  0.44588538]\n",
      " [ 2.37822997  1.65329435  0.84253069  2.0997198   0.11364831  0.6979372 ]\n",
      " [ 0.15993196  0.4231479   0.85315472  0.44798643 -0.51900485 -0.30258381]] \n",
      "\n",
      "20 :\n",
      "loss: [[ 3.68229334  0.69925218  1.58197039  2.34895658 -0.27353646]\n",
      " [ 2.96512424  2.06532304  2.57907639  3.92308438 -0.05266966]\n",
      " [ 1.92360813  0.65484208  5.78285831  4.11783356 -0.91655098]\n",
      " [ 2.46234441  1.77060015  3.88958365  4.64803711 -0.17219185]\n",
      " [ 2.13682046  0.09181519  1.15216819  2.12477724  1.16947078]] \n",
      "\n",
      "alpha_hat: [[ 0.91787479  0.50420788  0.9117678   0.24611105 -0.51955373  1.94181257]\n",
      " [ 1.26456562  1.84631031  1.32743457  1.15849592 -0.20243466  0.87445276]\n",
      " [ 2.26176111  0.97195161 -0.47986314  2.32626333 -1.86356586  0.5911487 ]\n",
      " [ 2.11231752  1.32185707  0.66356206  1.67256278 -0.16713195  0.29189029]\n",
      " [ 0.01911058  0.35060205  0.76650945  0.3179082  -0.59794719 -0.31942338]] \n",
      "\n",
      "40 :\n",
      "loss: [[ 2.45373708  0.64303595  0.84217257  0.57009226 -0.48258004]\n",
      " [ 1.65758638  1.58655952  1.74137481  1.72783232 -0.16958573]\n",
      " [ 0.66049703  0.54514883  4.87165031  2.26404877 -1.14432632]\n",
      " [ 0.98981382  1.13300344  2.86544588  2.38939765 -0.12902094]\n",
      " [ 0.55624586 -0.14531026  0.07617513  0.4900834   1.17387933]] \n",
      "\n",
      "alpha_hat: [[ 0.90949802  0.57405062  1.01063363  0.4820226  -0.33629918  2.1025025 ]\n",
      " [ 1.15246245  1.78152475  1.23024449  1.08409849 -0.2517145   0.85974582]\n",
      " [ 2.29033494  1.02507523 -0.34647278  2.45171844 -1.69937296  0.70397035]\n",
      " [ 1.93871735  1.13444431  0.54779966  1.43526871 -0.31675366  0.08511598]\n",
      " [-0.07649074  0.30738884  0.70849841  0.26051495 -0.62293515 -0.31454267]] \n",
      "\n",
      "60 :\n",
      "loss: [[ 2.06603842  0.66250698  0.49754777 -0.01748588 -0.58580868]\n",
      " [ 1.13426746  1.38936454  1.26757405  0.98332024 -0.20290604]\n",
      " [ 0.23545952  0.53372531  4.39612962  1.64181543 -1.27593015]\n",
      " [ 0.31460163  0.84364727  2.2359912   1.64794601 -0.0720604 ]\n",
      " [-0.11951657 -0.20837441 -0.54754978  0.0621442   1.20546232]] \n",
      "\n",
      "alpha_hat: [[ 0.88296538  0.61146706  1.06066987  0.62750898 -0.21902083  2.21130558]\n",
      " [ 1.07017042  1.74285453  1.1713039   1.04340733 -0.26892563  0.86428224]\n",
      " [ 2.29981981  1.06067145 -0.25446867  2.54076259 -1.57690899  0.79177348]\n",
      " [ 1.81802871  1.0267839   0.46727771  1.30195961 -0.39554639 -0.01327228]\n",
      " [-0.14731704  0.27906925  0.6650326   0.23823182 -0.62296226 -0.30075485]] \n",
      "\n",
      "80 :\n",
      "loss: [[ 1.96610215  0.68936105  0.33469468 -0.23369498 -0.64165284]\n",
      " [ 0.9178605   1.29586406  0.99217018  0.69723157 -0.2196478 ]\n",
      " [ 0.10713799  0.53611403  4.13628058  1.39722048 -1.3663948 ]\n",
      " [-0.01727012  0.68515698  1.84120204  1.36547914 -0.04310681]\n",
      " [-0.42040923 -0.22690365 -0.9175945  -0.03828806  1.22807829]] \n",
      "\n",
      "alpha_hat: [[ 0.84692525  0.62932126  1.08162378  0.71767506 -0.14280778  2.28728548]\n",
      " [ 1.00640144  1.71903835  1.13436645  1.0210098  -0.26938078  0.87811592]\n",
      " [ 2.29660232  1.08362834 -0.19138592  2.60341805 -1.48487026  0.86060066]\n",
      " [ 1.72857248  0.96368896  0.40683792  1.22585174 -0.43622958 -0.05286694]\n",
      " [-0.20352717  0.25856824  0.62941055  0.23295748 -0.61185751 -0.28411907]] \n",
      "\n",
      "100 :\n",
      "loss: [[ 1.95823363  0.70960949  0.25442236 -0.32906649 -0.67192206]\n",
      " [ 0.8240142   1.24510725  0.82475184  0.5623068  -0.23348137]\n",
      " [ 0.07759701  0.53352178  3.98232172  1.27304099 -1.43258469]\n",
      " [-0.19465974  0.58230883  1.58427309  1.22425582 -0.03660338]\n",
      " [-0.5601082  -0.23607223 -1.14394548 -0.05919627  1.24174621]] \n",
      "\n",
      "alpha_hat: [[ 0.80663419  0.6355825   1.08563925  0.77421101 -0.09217046  2.34236058]\n",
      " [ 0.95477124  1.70385002  1.11045456  1.00853198 -0.26135459  0.89593433]\n",
      " [ 2.28532631  1.09765359 -0.14826016  2.64714619 -1.41490236  0.9151191 ]\n",
      " [ 1.65829917  0.92580009  0.35821293  1.18136094 -0.45658066 -0.06079504]\n",
      " [-0.25030695  0.24237639  0.59841308  0.23591799 -0.59620712 -0.26744456]] \n",
      "\n",
      "120 :\n",
      "loss: [[ 1.97618871  0.722851    0.21161976 -0.38169892 -0.68719009]\n",
      " [ 0.78105591  1.21457319  0.71690526  0.48148883 -0.24643058]\n",
      " [ 0.07710756  0.52418814  3.88054687  1.19090029 -1.4818706 ]\n",
      " [-0.29805936  0.50692347  1.40905205  1.12926959 -0.04274361]\n",
      " [-0.62685026 -0.24429567 -1.28701857 -0.06604335  1.25055601]] \n",
      "\n",
      "alpha_hat: [[ 0.76520593  0.63513206  1.08010706  0.81041222 -0.05753663  2.38394551]\n",
      " [ 0.91157039  1.69379971  1.09453008  1.00138396 -0.24933311  0.91491654]\n",
      " [ 2.26926829  1.10546696 -0.11874774  2.67736802 -1.3609446   0.95883185]\n",
      " [ 1.60041678  0.90240358  0.31685376  1.1544394  -0.46626732 -0.05192207]\n",
      " [-0.29041753  0.22871982  0.57045974  0.24282352 -0.57913263 -0.25197912]] \n",
      "\n",
      "140 :\n",
      "loss: [[ 1.99910237  0.73121483  0.18613446 -0.41675889 -0.69345002]\n",
      " [ 0.76053522  1.19506605  0.64283115  0.42310003 -0.25839815]\n",
      " [ 0.08261964  0.50999594  3.805059    1.12641345 -1.51864721]\n",
      " [-0.36322721  0.44731132  1.28345995  1.0523442  -0.05469758]\n",
      " [-0.6585969  -0.25286542 -1.38027927 -0.07337614  1.25737564]] \n",
      "\n",
      "alpha_hat: [[ 0.72442233  0.63092712  1.06947628  0.83437379 -0.03301395  2.41664472]\n",
      " [ 0.87456915  1.68690074  1.08371327  0.99702985 -0.23574598  0.93358962]\n",
      " [ 2.25068427  1.10902051 -0.09841068  2.69796388 -1.31864709  0.99432788]\n",
      " [ 1.55102026  0.88752759  0.28022445  1.13734011 -0.47054364 -0.03439958]\n",
      " [-0.32542118  0.2166763   0.54474152  0.25155319 -0.56209375 -0.23822305]] \n",
      "\n",
      "160 :\n",
      "loss: [[ 2.02127822  0.73664596  0.16907991 -0.44287118 -0.69433715]\n",
      " [ 0.75069754  1.18234626  0.58868112  0.37616863 -0.26915743]\n",
      " [ 0.08747085  0.49302049  3.74349421  1.07163115 -1.54596567]\n",
      " [-0.40698603  0.3980022   1.18912535  0.98513741 -0.06855262]\n",
      " [-0.67262943 -0.26150129 -1.4426489  -0.08273005  1.26371834]] \n",
      "\n",
      "alpha_hat: [[ 0.68524983  0.62473815  1.05639078  0.85099574 -0.01498122  2.44332253]\n",
      " [ 0.84237039  1.68200194  1.07632146  0.99406018 -0.22189769  0.95121353]\n",
      " [ 2.23109183  1.10969415 -0.08417121  2.71168663 -1.28490475  1.02350723]\n",
      " [ 1.50780145  0.87781284  0.24688331  1.12577065 -0.47225582 -0.01269132]\n",
      " [-0.35627378  0.20574678  0.52081412  0.26104522 -0.54575036 -0.22632187]] \n",
      "\n",
      "180 :\n",
      "loss: [[ 2.04169241  0.74045655  0.15654828 -0.46335155 -0.69211545]\n",
      " [ 0.74636938  1.17419102  0.54688489  0.33646048 -0.27865038]\n",
      " [ 0.09039487  0.47481865  3.68997307  1.02366288 -1.56606109]\n",
      " [-0.43780966  0.35608953  1.11535818  0.92504486 -0.08228773]\n",
      " [-0.67734923 -0.26979698 -1.48514146 -0.09306339  1.27026466]] \n",
      "\n",
      "alpha_hat: [[ 6.48163487e-01  6.17610246e-01  1.04239525e+00  8.63228519e-01\n",
      "  -1.21073247e-03  2.46577146e+00]\n",
      " [ 8.14055220e-01  1.67842254e+00  1.07134503e+00  9.91689765e-01\n",
      "  -2.08472608e-01  9.67447615e-01]\n",
      " [ 2.21148568e+00  1.10845223e+00 -7.39104983e-02  2.72047963e+00\n",
      "  -1.25750450e+00  1.04776289e+00]\n",
      " [ 1.46934403e+00  8.71346857e-01  2.15986643e-01  1.11734044e+00\n",
      "  -4.72933679e-01  1.07703225e-02]\n",
      " [-3.83613754e-01  1.95646356e-01  4.98407784e-01  2.70761831e-01\n",
      "  -5.30375936e-01 -2.16255894e-01]] \n",
      "\n",
      "200 :\n",
      "loss: [[ 2.06052917  0.74342261  0.14678595 -0.47973261 -0.68821065]\n",
      " [ 0.74507233  1.16929126  0.51318373  0.30203581 -0.28695417]\n",
      " [ 0.09167118  0.45641924  3.64167957  0.98120601 -1.58060092]\n",
      " [-0.4602843   0.31983439  1.05576909  0.87104185 -0.0949669 ]\n",
      " [-0.67711578 -0.27750903 -1.5143913  -0.10332035  1.27725913]] \n",
      "\n",
      "alpha_hat: [[ 0.61334784  0.61014883  1.02837015  0.87284002  0.00967382  2.48512618]\n",
      " [ 0.78898661  1.67575043  1.06815865  0.98948303 -0.19581162  0.98216834]\n",
      " [ 2.19249616  1.10596239 -0.0661799   2.72571205 -1.2348667   1.06811981]\n",
      " [ 1.43473655  0.86702324  0.18702018  1.11071047 -0.47338663  0.0346747 ]\n",
      " [-0.40790487  0.18620127  0.47733859  0.28042917 -0.51605706 -0.20793184]] \n",
      "\n",
      "result: [[ 0.61334784  0.61014883  1.02837015  0.87284002  0.00967382  2.48512618]\n",
      " [ 0.78898661  1.67575043  1.06815865  0.98948303 -0.19581162  0.98216834]\n",
      " [ 2.19249616  1.10596239 -0.0661799   2.72571205 -1.2348667   1.06811981]\n",
      " [ 1.43473655  0.86702324  0.18702018  1.11071047 -0.47338663  0.0346747 ]\n",
      " [-0.40790487  0.18620127  0.47733859  0.28042917 -0.51605706 -0.20793184]]\n"
     ]
    }
   ],
   "source": [
    "model.infer_aspect_weight() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
