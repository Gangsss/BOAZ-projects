{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Chankoo\\\\Desktop\\\\GitHub\\\\BOAZ-projects\\\\airbnb-NLP'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./../airbnb-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pos_review.json') as fp:\n",
    "    review = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30569"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_lst = list(review.keys())\n",
    "sampled_key_lst = key_lst[:100]\n",
    "review2 = {}\n",
    "for k in sampled_key_lst:\n",
    "    review2[k] = review[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('voca_freq_dic.json') as fp:\n",
    "    voca_freq_dic = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223731"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voca_freq_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca_freq_dic2 = {}\n",
    "for k,v in voca_freq_dic.items():\n",
    "    if v>9:\n",
    "        voca_freq_dic2[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voca_freq_dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del voca_freq_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel.load('pass_10_iter_3000.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pass_10_iter_3000.lda.id2word','rb') as fp:\n",
    "    id2word = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'room_N'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word.id2token.get(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_wordid = []\n",
    "for k in range(K):\n",
    "    wid, p = zip(*ldamodel.get_topic_terms(k,topn=30))\n",
    "    aspect_wordid.append(wid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_keywords = []\n",
    "for kth_keys in aspect_wordid:\n",
    "    kth_keywords = []\n",
    "    for wid in kth_keys:\n",
    "        kth_keywords.append(id2word.id2token.get(wid))\n",
    "    aspect_keywords.append(kth_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aspect_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_terms(aspects, vocab_dict):\n",
    "    aspect_terms = []\n",
    "    w_notfound = []\n",
    "    \n",
    "    for aspect_kws in aspects:\n",
    "        aspect = []\n",
    "        for w in aspect_kws:\n",
    "            if w in vocab_dict:\n",
    "                aspect.append(w)\n",
    "            else:\n",
    "                w_notfound.append(w)\n",
    "        aspect_terms.append(aspect)\n",
    "    #We are only using one hotel review file, as we keep inceasing the number of files words not found will decrease.\n",
    "    # print \"Words not found in vocab:\", ' '.join(w_notfound)\n",
    "    return aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_aspect_terms(aspect_keywords,voca_freq_dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sq(a,b,c,d):\n",
    "    c1 = a\n",
    "    c2 = b - a\n",
    "    c3 = c - a\n",
    "    c4 = d - b - c + a\n",
    "    nc =  d\n",
    "    return nc * (c1*c4 - c2*c3) * (c1*c4 - c2*c3)/((c1+c3) * (c2+c4) * (c1+c2) * (c3+c4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sq_mat():\n",
    "    global aspect_words, aspect_sent, num_words\n",
    "    asp_rank = np.zeros(aspect_words.shape)\n",
    "    for i in range(len(aspect_terms)):\n",
    "        for j in range(len(vocab)):\n",
    "            asp_rank[i][j] = chi_sq(aspect_words[i][j], num_words[j], aspect_sent[i], len(sent))\n",
    "    return asp_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_terms = get_aspect_terms(aspect_keywords,voca_freq_dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 30, 30, 30, 29, 30]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len,aspect_terms)) # aspect keywords는 5번 aspect의 1단어빼고 voca_freq_dic2에 포함됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_segmentaion(review_dic,voca_freq_dic):\n",
    "    #INPUT\n",
    "    review_sent = [] # 문장, 단어단위로 토크나이징된 리뷰 담은 리스트\n",
    "    for rev_lst in review_dic.values():\n",
    "        for rev in rev_lst:\n",
    "            review_sent.append(rev[1])\n",
    "    \n",
    "    all_ratings = []\n",
    "    for rev_lst in review_dic.values():\n",
    "        for rev in rev_lst:\n",
    "            all_ratings.append(rev[0])\n",
    "\n",
    "    #selection threshold\n",
    "    p = 5\n",
    "    \n",
    "    #Iterations \n",
    "    # I = 10\n",
    "    I = 1\n",
    "\n",
    "#     #Create Vocabulary\n",
    "#     #review_sent, review_actual, only_sent = parse_to_sentence(reviews)\n",
    "#     #vocab, #vocab_dict = create_vocab(only_sent)\n",
    "\n",
    "    vocab = list(voca_freq_dic.keys())\n",
    "    #Assign a number corresponding to each word. Makes counting easier.\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab)))) # word를 key 로 word의 인덱스를 value로  \n",
    "\n",
    "    #Aspect Keywords\n",
    "    aspect_terms = get_aspect_terms(aspect_keywords,voca_freq_dic)\n",
    "\n",
    "#     label_text = \n",
    "    # print aspect_terms\n",
    "\n",
    "    #ALGORITHM\n",
    "    review_labels = []\n",
    "    k = len(aspect_terms) # k: 토픽 개수 10\n",
    "    v = len(vocab) # v: 단어 개수 18923\n",
    "    \n",
    "    aspect_words = np.zeros((k,v))\n",
    "    aspect_sent = np.zeros(k)\n",
    "    num_words = np.zeros(v)\n",
    "    #-----------------------------------------------------\n",
    "    for _ in tqdm.tqdm(range(I)):\n",
    "        for r in review_sent:\n",
    "            labels = []\n",
    "            for s in r:\n",
    "                count = np.zeros(len(aspect_terms)) # 한 문장에 대해 aspect keyword가 몇개씩있는지 카운트\n",
    "                \n",
    "                for i,a in enumerate(aspect_terms):\n",
    "                    for w in s:\n",
    "                        if w in vocab_dict:\n",
    "                            num_words[vocab_dict[w]] += 1\n",
    "                            if w in a:\n",
    "                                count[i] += 1\n",
    "\n",
    "                if max(count) > 0:\n",
    "                    la = np.where(np.max(count) == count)[0].tolist() # count 중 max인 aspect들의  인덱스 뽑아 리스트로\n",
    "                    \n",
    "                    labels.append(la)\n",
    "                    for idx in la:\n",
    "                        aspect_sent[idx] += 1\n",
    "                        for w in s:\n",
    "                            if w in vocab_dict:\n",
    "                                aspect_words[idx][vocab_dict[w]] += 1\n",
    "                else:\n",
    "                    labels.append([])\n",
    "                    \n",
    "            review_labels.append(labels)\n",
    "        \n",
    "#             aspect_w_rank = chi_sq_mat()\n",
    "#             new_labels = []\n",
    "#             for na in aspect_w_rank:\n",
    "#                 x = np.argsort(na)[::-1][:p]\n",
    "#                 new_labels.append(x)\n",
    "#                 for k,v in vocab_dict.items():\n",
    "#                     if vocab_dict[k] in x:\n",
    "#                         print(k)\n",
    "#             sys.exit()\n",
    "            \n",
    "    return review_labels,review_sent\n",
    "\n",
    "#     ratings_sentiment = []\n",
    "#     for r in review_actual:\n",
    "#         sentiment = []\n",
    "#         #aspect ratings based on sentiment\n",
    "#         for s in r:\n",
    "#             ss = sid.polarity_scores(s)\n",
    "#             sentiment.append(ss['compound'])\n",
    "#         ratings_sentiment.append(sentiment)\n",
    "\n",
    "#     #Aspect Ratings Per Review\n",
    "#     aspect_ratings = []\n",
    "#     for i,r in enumerate(review_labels):\n",
    "#         rating = np.zeros(7)\n",
    "#         count = np.zeros(7)\n",
    "#         rs = ratings_sentiment[i] \n",
    "#         for j,l in enumerate(r):\n",
    "#             for k in range(7):\n",
    "#                 if k in l:\n",
    "#                     rating[k] += rs[j]\n",
    "#             for k in range(7):\n",
    "#                 if count[k] != 0:\n",
    "#                     rating[k] /= count[k]\n",
    "#         #Map from -[-1,1] to [1,5]\n",
    "#         for k in range(7):\n",
    "#             if rating[k] != 0:\n",
    "#                 rating[k] = int(round((rating[k]+1)*5/2))\n",
    "#         aspect_ratings.append(rating)\n",
    "#     return aspect_ratings, all_ratings\n",
    "\n",
    "    # n = 0\n",
    "    # print review_actual[n], '\\n', review_labels[n]\n",
    "    # print ratings_sentiment[n], '\\n', aspect_ratings[n]\n",
    "    # print len(all_ratings), len(reviews), all_ratings[0]\n",
    "    # sys.exit()\n",
    "    # return aspect_ratings\n",
    "\n",
    "    # print sent[5:9], labels[5:9]\n",
    "    # print zip(actual_sent, labels)[:10]\n",
    "    # print zip(actual_sent, sentiment)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "omit_word = []\n",
    "for k,rev_lst in review.items():\n",
    "    for rev in rev_lst:\n",
    "        for i,sent in enumerate(rev[1]):\n",
    "            sent_tmp =[]\n",
    "            for word in sent:\n",
    "                if word in voca_freq_dic2:\n",
    "                    sent_tmp.append(word)\n",
    "                else:\n",
    "                    omit_word.append(word)\n",
    "            rev[1][i] = sent_tmp\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "review_labels2,review_sents2 = aspect_segmentaion(review2,voca_freq_dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_labels2 = np.asarray(review_labels2)\n",
    "review_sents2 = np.array(review_sents2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('review_labels.lst','wb') as fp:\n",
    "#     pickle.dump(review_labels,fp)\n",
    "# with open('review_sents.lst','wb') as fp:\n",
    "#     pickle.dump(review_sent,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_labels2[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room_N',\n",
       " 'spacious_J',\n",
       " 'spacious_J',\n",
       " 'bedroom_N',\n",
       " 'see_V',\n",
       " 'windowwhen_V',\n",
       " 'go_V',\n",
       " 'caught_J',\n",
       " 'typhoon_N',\n",
       " 'fortunately_R',\n",
       " 'room_N',\n",
       " 'occupy_V',\n",
       " 'doubledecked_J',\n",
       " 'windowsthe_N',\n",
       " 'room_N',\n",
       " 'modern_J',\n",
       " 'technologicaloverall_N',\n",
       " 'pretty_R',\n",
       " 'good_J']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sents2[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.43307602),\n",
       " (1, 0.043798342),\n",
       " (2, 0.30630082),\n",
       " (3, 0.13603733),\n",
       " (4, 0.02652961),\n",
       " (5, 0.054257885)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA 의 예측과 비교\n",
    "\n",
    "doc = review_sents2[1][0]\n",
    "bow = ldamodel.id2word.doc2bow(doc)\n",
    "ldamodel[bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리뷰에 대해 aspect별로 word-frequency matrix를 만들어주자( *W_d_ij* )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_cnt = 0\n",
    "for rev_lst in review2.values():\n",
    "    review_cnt += len(rev_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5543"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744447072"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = review_cnt # 리뷰수\n",
    "K = len(aspect_terms) # aspect 수\n",
    "N = len(voca_freq_dic2) # 단어수\n",
    "D*K*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-ebe7ff6ec6bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'int16'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# W = np.ndarray((D,K,N),dtype = 'int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = {} # K로 나눠 딕셔너리 만들자\n",
    "for i in range(K):\n",
    "    W['w_'+str(i)] = np.ndarray((D,N),dtype = 'int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w_0': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16),\n",
       " 'w_1': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16),\n",
       " 'w_2': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16),\n",
       " 'w_3': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16),\n",
       " 'w_4': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16),\n",
       " 'w_5': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16)}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kth_count(sent_labels, sents,k,freq_dic_kth):\n",
    "    for idx,label in enumerate(sent_labels):\n",
    "        if k in label: # kth aspect이면\n",
    "            for word in sents[idx]:\n",
    "                if word in freq_dic_kth:\n",
    "                    freq_dic_kth[word] += 1\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "\n",
    "def make_kth_mat(k):\n",
    "    global W,D # word freq 담을 전역변수 W. kth aspect 의 freq은 w_k (np.array)에 담는다/ 리뷰개수 D\n",
    "    for d in tqdm.tqdm(range(D)):# 리뷰개수만큼 돌면서\n",
    "        freq_dic_kth = dict.fromkeys(voca_freq_dic2.keys(),0) # kth freq dic 초기화\n",
    "        kth_count(review_labels2[d],review_sents2[d],k,freq_dic_kth) # freq_dic_kth 을 완성\n",
    "        words,freqs = zip(*sorted(freq_dic_kth.items(),key = lambda kv:kv[0])) # freq_dic_kth를 사전순으로 정렬해 단어와 freq를 각각 뽑는다\n",
    "        W['w_'+str(k)][d] = np.array(freqs,dtype='int16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5543/5543 [07:06<00:00, 12.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5543/5543 [06:59<00:00, 13.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5543/5543 [05:09<00:00, 17.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5543/5543 [04:11<00:00, 22.01it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5543/5543 [04:12<00:00, 21.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 5543/5543 [04:10<00:00, 22.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in range(K):\n",
    "    make_kth_mat(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('W_6_100.mat','wb') as fp:\n",
    "#     pickle.dump(W,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('W_6_100.mat','rb') as fp:\n",
    "    W = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th aspect\n",
      "['room_N', 'house_N', 'nice_J', 'clean_J', 'really_R']\n",
      "sum of freq of words __ 38579 __\n",
      "1th aspect\n",
      "['station_N', 'convenient_N', 'restaurant_N', 'easy_J', 'area_N']\n",
      "sum of freq of words __ 22083 __\n",
      "2th aspect\n",
      "['place_N', 'great_J', 'good_J', 'location_N', 'stay_N']\n",
      "sum of freq of words __ 34118 __\n",
      "3th aspect\n",
      "['well_R', 'everything_N', 'need_V', 'small_J', 'people_N']\n",
      "sum of freq of words __ 15631 __\n",
      "4th aspect\n",
      "['take_V', 'check_N', 'check_V', 'wifi_N', 'arrive_V']\n",
      "sum of freq of words __ 7347 __\n",
      "5th aspect\n",
      "['host_N', 'get_V', 'kind_N', 'lot_N', 'provide_V']\n",
      "sum of freq of words __ 17506 __\n"
     ]
    }
   ],
   "source": [
    "for i in range(K): # k aspect별로 단어 freq의 합을 찍어보자\n",
    "    print('{}th aspect'.format(i))\n",
    "    print(aspect_keywords[i][:5])\n",
    "    print('sum of freq of words __',np.sum(W['w_'+str(i)]),'__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
